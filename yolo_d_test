#! usr/bin/env python3

import pyrealsense2 as rs
import numpy as np
import cv2
import torch

# Load Yolo v5.
# use yolov5s (small) if you want better detection.
# but yolov5n (nano) works well, and 5s has bit of delay.
model = torch.hub.load('ultralytics/yolov5', 'yolov5n')  

# RealSense 파이프라인 초기화
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)  
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)   

pipeline.start(config)

align_to = rs.stream.color
align = rs.align(align_to)

# Set Parameters for filtering. (필터링 기준들)
CONFIDENCE_THRESHOLD = 0.4  # minimum confidence threshold.
ALLOWED_CLASSES = ['person', 'car', 'motorcycle' ,'dog', 'cat', 'bus', 'truck', 'bicycle'] 
MIN_WIDTH, MIN_HEIGHT = 50, 50  # Minimum size of bounding box.

"""
List of classes. based on COCO dataset. (80 classses available)
['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',
 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',
 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',
 'TV', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
 'hair drier', 'toothbrush']

"""

try:
    while True:
        frames = pipeline.wait_for_frames()
        aligned_frames = align.process(frames)

        color_frame = aligned_frames.get_color_frame()
        depth_frame = aligned_frames.get_depth_frame()

        color_image = np.asanyarray(color_frame.get_data())
        depth_image = np.asanyarray(depth_frame.get_data())

        results = model(color_image)  
        
        for *xyxy, conf, cls in results.xyxy[0]:  
            if conf < CONFIDENCE_THRESHOLD:  
                continue
            class_name = model.names[int(cls)]  
            if class_name not in ALLOWED_CLASSES:  
                continue

            x1, y1, x2, y2 = map(int, xyxy)  
            width, height = x2 - x1, y2 - y1
            if width < MIN_WIDTH or height < MIN_HEIGHT:  
                continue

            # 바운딩 박스 중앙 좌표
            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
            depth_value = depth_image[center_y, center_x]  
            depth_cm = depth_value / 10.0  

            cv2.rectangle(color_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(color_image, f"{class_name} {conf:.2f}", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            depth_text = f"Depth: {depth_cm:.1f} cm"
            cv2.putText(color_image, depth_text, (center_x - 40, center_y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

            cv2.circle(color_image, (center_x, center_y), 5, (0, 0, 255), -1)

        depth_colormap = cv2.applyColorMap(
            cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET
        )

        combined_image = np.hstack((color_image, depth_colormap))
        cv2.imshow('RealSense - Filtered YOLOv5 Detection', combined_image)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

finally:
    pipeline.stop()
    cv2.destroyAllWindows()

